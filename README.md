# ğŸ›’ E-Commerce Data Pipeline Project

> **Complete ELT (Extract, Load, Transform) pipeline with automated orchestration using Apache Airflow**

This project implements a comprehensive data pipeline for analyzing Brazilian e-commerce data from Olist. It includes both **manual execution** and **automated orchestration** approaches, demonstrating modern data engineering practices.

## ğŸ“Š Project Overview

### Business Problem
Analyze e-commerce data to understand:
- Revenue patterns by state and time
- Delivery performance vs estimates  
- Product category performance
- Customer behavior and geography

### Data Source
- **Primary**: Brazilian E-Commerce Public Dataset by Olist (9 CSV files)
- **Secondary**: Brazil Public Holidays API
- **Output**: SQLite database with transformed analytics tables

---

## ğŸ—ï¸ Architecture Overview

```mermaid
graph LR
    A[CSV Files + API] --> B[Extract]
    B --> C[Load to SQLite]
    C --> D[Transform with SQL]
    D --> E[Analytics & Viz]
    
    F[Manual Execution] --> B
    G[Airflow Orchestration] --> B
    
    style F fill:#ff9800
    style G fill:#4caf50
```

### Two Execution Methods:

1. **ğŸ”§ Manual Pipeline** - Run steps individually for development/testing
2. **âš¡ Automated Orchestration** - Scheduled, monitored, and reliable production pipeline

---

## ğŸ“Š Dataset Setup

**âš ï¸ The dataset files are not included in this repository due to size constraints (126MB+).**

**Download the Olist E-Commerce Dataset:**

1. **Visit**: [Olist Brazilian E-Commerce Public Dataset by AndrÃ© Sionek on Kaggle](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)
2. **Download** all CSV files (sign in to Kaggle required)
3. **Create** a `dataset/` folder in the project root
4. **Place** all CSV files in the `dataset/` folder

**Required files:**
- `olist_customers_dataset.csv`
- `olist_geolocation_dataset.csv`  
- `olist_order_items_dataset.csv`
- `olist_order_payments_dataset.csv`
- `olist_order_reviews_dataset.csv`
- `olist_orders_dataset.csv`
- `olist_products_dataset.csv`
- `olist_sellers_dataset.csv`
- `product_category_name_translation.csv`

---

## ğŸš€ Quick Start Guide

### Prerequisites
- Python 3.8+
- Docker Desktop (for orchestration)
- Git
- **Dataset files** (see Dataset Setup above)

### Option 1: Automated Pipeline (Recommended)

**Start the complete automated pipeline in 2 commands:**

```bash
# 1. Navigate to airflow directory
cd airflow

# 2. Start Airflow with Docker
docker-compose -f docker-compose-simple.yml up -d
```

**Access the Web UI:**
- Open: http://localhost:8080
- Login: `admin` / `admin`
- Click `ecommerce_etl_pipeline` â†’ "Trigger DAG"

### Option 2: Manual Pipeline

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Run pipeline steps manually
python -c "from src.extract import extract; extract()"
python -c "from src.load import load; load()"  
python -c "from src.transform import run_queries; run_queries()"

# 3. Open Jupyter notebook for analysis
jupyter notebook "AnyoneAI - Sprint Project 01.ipynb"
```

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ ğŸ“‚ airflow/                    # ğŸ†• AUTOMATED ORCHESTRATION
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ ecommerce_etl_dag.py   # Main automated pipeline
â”‚   â”‚   â””â”€â”€ simple_ecommerce_dag.py # Demo pipeline
â”‚   â”œâ”€â”€ operators/
â”‚   â”‚   â””â”€â”€ etl_operators.py       # Custom Airflow operators
â”‚   â”œâ”€â”€ docker-compose-simple.yml  # Docker setup
â”‚   â””â”€â”€ logs/                      # Execution logs
â”‚
â”œâ”€â”€ ğŸ“‚ src/                        # ğŸ“‹ CORE PIPELINE CODE
â”‚   â”œâ”€â”€ extract.py                 # Data extraction logic
â”‚   â”œâ”€â”€ load.py                    # Database loading logic
â”‚   â”œâ”€â”€ transform.py               # SQL query execution
â”‚   â”œâ”€â”€ config.py                  # Configuration settings
â”‚   â””â”€â”€ plots.py                   # Visualization functions
â”‚
â”œâ”€â”€ ğŸ“‚ dataset/                    # ğŸ“Š RAW DATA
â”‚   â”œâ”€â”€ olist_customers_dataset.csv
â”‚   â”œâ”€â”€ olist_orders_dataset.csv
â”‚   â””â”€â”€ ... (9 CSV files total)
â”‚
â”œâ”€â”€ ğŸ“‚ queries/                    # ğŸ” SQL TRANSFORMATIONS
â”‚   â”œâ”€â”€ revenue_by_month_year.sql
â”‚   â”œâ”€â”€ top_10_revenue_categories.sql
â”‚   â””â”€â”€ ... (7 SQL files)
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                      # âœ… QUALITY ASSURANCE
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â””â”€â”€ query_results/
â”‚
â”œâ”€â”€ AnyoneAI - Sprint Project 01.ipynb  # ğŸ“ˆ ANALYSIS NOTEBOOK
â”œâ”€â”€ olist.db                      # ğŸ’¾ Generated SQLite database
â””â”€â”€ requirements.txt               # ğŸ“¦ Python dependencies
```

---

## ğŸ”„ Pipeline Flow Detailed

### Phase 1: Extract ğŸ“¥
**Source**: `src/extract.py`
- Reads 9 CSV files from `/dataset` folder
- Fetches Brazil public holidays from API for 2017
- Validates data quality and structure
- **Output**: Python DataFrames in memory

**Data Sources:**
- Customer demographics and locations
- Order details and timestamps  
- Product information and categories
- Payment and shipping data
- Customer reviews and ratings

### Phase 2: Load ğŸ“¤
**Source**: `src/load.py`
- Creates SQLite database (`olist.db`)
- Loads all DataFrames into database tables
- Maintains referential integrity
- **Output**: Normalized database ready for analysis

**Database Tables:**
- `customers`, `orders`, `order_items`
- `products`, `sellers`, `payments`
- `reviews`, `geolocation`, `holidays`

### Phase 3: Transform ğŸ”„
**Source**: `src/transform.py` + `queries/*.sql`
- Executes 7 analytical SQL queries
- Generates business intelligence tables
- Calculates metrics and KPIs
- **Output**: Analytics tables for visualization

**Analytics Generated:**
- Monthly/yearly revenue trends
- State-wise revenue distribution
- Top/bottom product categories
- Delivery performance analysis
- Customer satisfaction metrics

### Phase 4: Analyze & Visualize ğŸ“Š
**Source**: Jupyter notebook
- Interactive data exploration
- Statistical analysis and insights
- Professional visualizations
- Business recommendations

---

## âš¡ Airflow Orchestration Features

### ğŸ¯ What Makes It Production-Ready

**ğŸ• Scheduling**
- Runs automatically daily at 2:00 AM
- No manual intervention required
- Configurable schedule patterns

**ğŸ”„ Reliability**
- 2 automatic retries with 5-minute delays
- Task dependencies ensure correct execution order
- Failure notifications and error handling

**ğŸ“Š Monitoring**
- Real-time web dashboard
- Task execution logs and metrics
- Performance tracking and alerting

**ğŸ”§ Maintenance**
- Version control for pipeline code
- Easy configuration management
- Rollback capabilities

### ğŸŒ Web Interface Features

Access http://localhost:8080 to:
- **ğŸ“ˆ Dashboard**: Overview of all pipelines
- **ğŸ” DAG View**: Visual pipeline representation  
- **ğŸ“‹ Task Logs**: Detailed execution information
- **âš™ï¸ Configuration**: Pipeline settings and schedules
- **ğŸ“Š Metrics**: Performance and success rates

---

## ğŸ› ï¸ Development & Testing

### Run Tests
```bash
# Test data extraction
pytest tests/test_extract.py

# Test data transformations  
pytest tests/test_transform.py

# Test all components
pytest tests/
```

### Manual Development Workflow
```bash
# 1. Modify source code in src/
# 2. Test individual components
python -m src.extract
python -m src.load  
python -m src.transform

# 3. Verify results in database
sqlite3 olist.db ".tables"
```

### Airflow Development
```bash
# Check DAG syntax
cd airflow
docker exec airflow_standalone python -m py_compile /opt/airflow/dags/ecommerce_etl_dag.py

# View DAG structure  
docker exec airflow_standalone airflow dags list

# Test individual tasks
docker exec airflow_standalone airflow tasks test ecommerce_etl_pipeline extract_data 2024-01-01
```

---

## ğŸ³ Docker Management

### Start Pipeline
```bash
cd airflow
docker-compose -f docker-compose-simple.yml up -d
```

### Monitor Logs
```bash
# View all logs
docker logs airflow_standalone

# Follow logs in real-time
docker logs airflow_standalone -f

# Check container status
docker-compose -f docker-compose-simple.yml ps
```

### Stop Pipeline
```bash
docker-compose -f docker-compose-simple.yml down
```

### Troubleshooting
```bash
# Restart containers
docker-compose -f docker-compose-simple.yml restart

# Clean restart (removes data)
docker-compose -f docker-compose-simple.yml down -v
docker-compose -f docker-compose-simple.yml up -d
```

---

## ğŸ“ˆ Key Insights & Analytics

This pipeline generates insights on:

### ğŸ’° Revenue Analysis
- Monthly revenue trends (2016-2018)
- Top 10 revenue-generating states
- Product category performance
- Seasonal patterns and growth

### ğŸšš Logistics Performance  
- Delivery time vs estimates by state
- Shipping performance metrics
- Geographic distribution analysis

### ğŸ›ï¸ Product Intelligence
- Top/bottom performing categories
- Order volume and value correlation
- Product popularity trends

### ğŸ‘¥ Customer Behavior
- Geographic customer distribution
- Review patterns and satisfaction
- Purchase timing and preferences

---

## ğŸ¯ Business Value

### For Data Engineers
- **Production-ready pipeline** with monitoring and scheduling
- **Modern best practices** using Docker and Airflow
- **Scalable architecture** that can handle larger datasets
- **Automated testing** and quality assurance

### for Business Analysts
- **Reliable daily updates** without manual intervention
- **Rich analytics dataset** ready for BI tools
- **Historical trend analysis** with consistent data quality
- **Self-service access** to pipeline status and logs

### For Data Scientists
- **Clean, validated datasets** for modeling
- **Reproducible data preparation** process
- **Feature engineering pipeline** ready for ML
- **A/B testing framework** for experiments

---

## ğŸš€ Next Steps & Extensions

### Possible Enhancements
- **ğŸ”„ Real-time streaming** with Apache Kafka
- **â˜ï¸ Cloud deployment** on AWS/GCP/Azure
- **ğŸ“Š Advanced analytics** with machine learning
- **ğŸ” Security hardening** with authentication
- **ğŸ“ˆ Performance optimization** for larger datasets
- **ğŸ”” Alert systems** for business KPIs

### Production Considerations
- **Database scaling** to PostgreSQL/Snowflake
- **Data validation** with Great Expectations  
- **Monitoring** with Prometheus/Grafana
- **CI/CD pipeline** for code deployment
- **Data lineage** tracking and documentation

---

## ğŸ‰ Achievement Summary

This project demonstrates:
- âœ… **Complete ELT pipeline** from raw data to insights
- âœ… **Production orchestration** with Apache Airflow
- âœ… **Modern containerization** with Docker
- âœ… **Automated scheduling** and monitoring
- âœ… **Quality assurance** with comprehensive testing
- âœ… **Business intelligence** with actionable insights

**From manual scripts to enterprise-grade automation - this is how modern data engineering works!** ğŸš€

---

## ğŸ“ Support & Documentation

- **Pipeline Issues**: Check Airflow logs at http://localhost:8080
- **Data Questions**: Review SQL queries in `/queries` folder  
- **Development**: Follow testing procedures in `/tests`
- **Business Logic**: See analysis notebook for context

**Happy Data Engineering! ğŸŠ**
